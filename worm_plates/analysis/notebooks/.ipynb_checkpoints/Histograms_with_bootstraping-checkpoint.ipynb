{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tierpsy.features.tierpsy_features import timeseries_feats_columns, ventral_signed_columns\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import numba\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from scipy.special import comb\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "all_feats = timeseries_feats_columns\n",
    "all_feats = [x for x in all_feats if not 'path_curvature' in x]\n",
    "\n",
    "def digitize_features(centered_data, ybins):\n",
    "    digitized_data = {}\n",
    "    for feat in all_feats:\n",
    "        ybin = ybins[feat]\n",
    "        bot, top = ybin[0], ybin[-1]\n",
    "        dat = np.clip(centered_data[feat], bot + 1e-6, top - 1e-6)\n",
    "        \n",
    "        yedges = np.concatenate([ybin - (ybin[1]-ybin[0])/2, [np.inf]])\n",
    "        digitized = np.digitize(dat, yedges) -1\n",
    "        \n",
    "        #flag bad rows with -1\n",
    "        counts[np.isnan(dat)] = -1\n",
    "        digitized_data[feat] = digitized\n",
    "    \n",
    "    return digitized_data\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def calc_histogram_2d(x_digit, y_digit, n_x, n_y):\n",
    "    H = np.zeros((n_y, n_x), np.int32)\n",
    "    for ii in range(y_digit.size):\n",
    "        x = x_digit[ii]\n",
    "        y = y_digit[ii]\n",
    "        if y >= 0:\n",
    "            H[y, x] += 1\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_timebin(df, xbins, reduce_func = np.nanmedian, bootstraping_size = 100):\n",
    "\n",
    "    ts_data = df.to_records(index = False)\n",
    "\n",
    "    w_groups_index = defaultdict(list)\n",
    "    for ii, w in enumerate(ts_data['worm_index']):\n",
    "        w_groups_index[w].append(ii)\n",
    "\n",
    "\n",
    "    w_groups = {}\n",
    "    worms_in_time = defaultdict(list)\n",
    "    for w, w_inds in w_groups_index.items():\n",
    "        w_data = ts_data[w_inds]\n",
    "\n",
    "        t_groups = defaultdict(list)\n",
    "        for ii, t in enumerate(w_data['time_binned']):\n",
    "            t_groups[t].append(ii)\n",
    "\n",
    "        w_groups[w] = {}\n",
    "        for t, t_inds in t_groups.items():\n",
    "            w_groups[w][t] = w_data[t_inds]\n",
    "            worms_in_time[t].append(w)\n",
    "\n",
    "    def get_average_data(worms2use_dict):        \n",
    "        avg_data = []\n",
    "        for t in range(len(xbins)):\n",
    "            w_data = []\n",
    "            \n",
    "            worms2use = worms2use_dict[t]\n",
    "            for w in worms2use:\n",
    "                if t in w_groups[w]:\n",
    "                    w_data.append(w_groups[w][t])\n",
    "            if w_data:\n",
    "                w_data = np.concatenate(w_data)\n",
    "                dd = [reduce_func(w_data[x]) for x in w_data.dtype.names]\n",
    "            else:\n",
    "                dd = [np.nan for _ in ts_data.dtype.names]\n",
    "            avg_data.append(dd)\n",
    "\n",
    "        return avg_data\n",
    "\n",
    "    avg_data = get_average_data(worms_in_time)\n",
    "    df_stat = pd.DataFrame(avg_data, columns = df.columns, index = xbins)\n",
    "    \n",
    "    \n",
    "    bootstraping_data = []\n",
    "    for _ in range(bootstraping_size):\n",
    "        worms2use_dict = {}\n",
    "        for t, available_worms in worms_in_time.items():\n",
    "            worms2use_dict[t] = np.random.choice(available_worms, len(available_worms), replace=True)\n",
    "        dat = get_average_data(worms2use_dict)\n",
    "        bootstraping_data.append(dat)\n",
    "\n",
    "    bootstraping_data = np.array(bootstraping_data)\n",
    "    \n",
    "    err_low, err_high = np.percentile(bootstraping_data, [5, 95], axis=0)\n",
    "    df_err_low = pd.DataFrame(err_low, columns = df.columns, index = xbins)\n",
    "    df_err_high = pd.DataFrame(err_high, columns = df.columns, index = xbins)\n",
    "    \n",
    "    #err = np.std(bootstraping_data, axis=0)\n",
    "    #df_err = pd.DataFrame(err, columns = df.columns, index = xbins)\n",
    "    \n",
    "    return df_stat, (df_err_low, df_err_high)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR(x):\n",
    "    q = x.quantile([0.25, 0.75])\n",
    "    inter_q = q.loc[0.75] - q.loc[0.25]\n",
    "    return inter_q\n",
    "\n",
    "def read_data(src_file, \n",
    "              events_dir, \n",
    "              strains2read, \n",
    "              xbins = np.arange(-5, 5.1, 0.5), \n",
    "              qbins = (0.01, 0.99),\n",
    "              fps = 25,\n",
    "              abs_ventral_features = True\n",
    "             ):\n",
    "    \n",
    "    xedges = np.concatenate([xbins - (xbins[1]-xbins[0])/2, [np.inf]])\n",
    "    \n",
    "    files_data = pd.read_pickle(src_file)\n",
    "    strain_basenames = {s : x for s,x in files_data.groupby('strain')}\n",
    "    \n",
    "    strains_data = {}\n",
    "    strain_avg_data = {}\n",
    "    bin_ranges_l = []\n",
    "    for istrain, strain in enumerate(tqdm_notebook(strains2read)):\n",
    "        basenames = strain_basenames[strain]['basename'].values\n",
    "        \n",
    "        all_df = []\n",
    "        \n",
    "        tot_worms = 0\n",
    "        for bn in basenames:\n",
    "            ts_file = events_dir / f'{bn}_timeseries.pkl'\n",
    "            if ts_file.exists():\n",
    "                df_ts = pd.read_pickle(ts_file)\n",
    "                \n",
    "                w_inds = np.unique(df_ts['worm_index'].values)\n",
    "                w_mapping = {k : (i + tot_worms) for i,k in enumerate(w_inds)}\n",
    "                tot_worms += len(w_inds)\n",
    "                \n",
    "                df_ts['worm_index_original'] = df_ts['worm_index']\n",
    "                df_ts['worm_index'] = df_ts['worm_index'].map(w_mapping)\n",
    "                \n",
    "                all_df.append(df_ts)\n",
    "                \n",
    "        all_df = pd.concat(all_df)\n",
    "    \n",
    "        if abs_ventral_features:\n",
    "            feats2abs = [x for x in ventral_signed_columns if x in all_df]\n",
    "            all_df[feats2abs] = all_df[feats2abs].abs()\n",
    "            \n",
    "        \n",
    "\n",
    "        all_df['time_centered'] = all_df['timestamp_centered']/fps\n",
    "        valid = (all_df['time_centered']>=xbins[0]) & (all_df['time_centered']<=xbins[-1])\n",
    "        all_df = all_df[valid].copy()\n",
    "        all_df['time_binned'] = np.digitize(all_df['time_centered'], xedges) -1\n",
    "        \n",
    "        strains_data[strain] = all_df\n",
    "        #df_g = all_df.groupby('time_binned')\n",
    "        #df_means = df_g.agg(['median', 'count'])\n",
    "        \n",
    "        \n",
    "        df_grouped = group_by_timebin(all_df, xbins)\n",
    "        strain_avg_data[strain] = df_grouped\n",
    "        \n",
    "        q = all_df.quantile(qbins)\n",
    "        bin_ranges_l.append(q)\n",
    "\n",
    "\n",
    "    q_bot = [q.loc[qbins[0]] for q in bin_ranges_l]\n",
    "    q_bot = pd.concat(q_bot, axis=1).min(axis=1)\n",
    "\n",
    "    q_top = [q.loc[qbins[1]] for q in bin_ranges_l]\n",
    "    q_top = pd.concat(q_top, axis=1).max(axis=1)\n",
    "\n",
    "    bin_ranges = pd.concat((q_bot, q_top), axis=1)\n",
    "    bin_ranges.columns = ['bot', 'top']\n",
    "    \n",
    "    \n",
    "    return strains_data, strain_avg_data, bin_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(strains_data,\n",
    "                  strain_avg_data,\n",
    "                  bin_ranges,\n",
    "                  feats2plot = ['length'], \n",
    "                  num_ybins = 15, \n",
    "                  xbins = np.arange(-10, 10.1, 1),\n",
    "                  min_valid_counts = 250,\n",
    "                  save_name = None\n",
    "                 ):\n",
    "\n",
    "    figs2return = []\n",
    "    \n",
    "    num_xbins = len(xbins)\n",
    "    xticks = [0.5, num_xbins//2 - .5, num_xbins-0.5]\n",
    "    xtickslabels = [int(xbins[0]), 0, int(xbins[-1])]\n",
    "    yticks = [0.5, num_ybins//2 - .5, num_ybins-0.5]\n",
    "    \n",
    "    figsize = (22, 2)\n",
    "    \n",
    "    strain_titles = {}\n",
    "    for strain, strain_ts_data in strains_data.items():\n",
    "        s_g = strain_ts_data[['worm_index','time_centered']].groupby('time_centered')\n",
    "        n_events = [len(dat['worm_index'].unique()) for t, dat in s_g]\n",
    "        strain_titles[strain] = f'{strain}={min(n_events)}-{max(n_events)}'\n",
    "    \n",
    "    \n",
    "    \n",
    "    for feat in tqdm_notebook(feats2plot):\n",
    "        feat_label = '\\n'.join(wrap(feat.replace('_', ' ').title(), 25))\n",
    "        \n",
    "        y_bot, y_top = bin_ranges.loc[feat].values\n",
    "        ybins = np.linspace(y_bot, y_top, num_ybins)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            fig_hist, axs = plt.subplots(1, len(strains_data), figsize = figsize, sharex = True, sharey = True)\n",
    "            figs2return.append(fig_hist)\n",
    "            \n",
    "        for ax, (strain, strain_ts_data) in zip(axs, strains_data.items()):\n",
    "            feat_data = strain_ts_data[feat]\n",
    "\n",
    "            dat = np.clip(feat_data, y_bot + 1e-6, y_top - 1e-6)\n",
    "            y_digit = np.digitize(dat, ybins)\n",
    "            x_digit = strain_ts_data['time_binned'].values\n",
    "\n",
    "            assert y_digit.size == x_digit.size\n",
    "            counts = calc_histogram_2d(x_digit, y_digit, num_xbins, num_ybins)\n",
    "            \n",
    "            N = counts.sum(axis=0)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                H = counts/N\n",
    "            H[:, N<min_valid_counts] = np.nan\n",
    "\n",
    "            ax.imshow(H, aspect = 'auto', origin='lower')\n",
    "            ax.set_title(strain_titles[strain])\n",
    "\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xticklabels(xtickslabels)\n",
    "        axs[0].set_ylabel(feat_label)\n",
    "\n",
    "\n",
    "\n",
    "        ytickslabels = [ybins[0], ybins[num_ybins//2] , ybins[-1]]\n",
    "        ytickslabels = [f'{x:.2f}' for x in ytickslabels]\n",
    "\n",
    "        axs[0].set_yticks(yticks)\n",
    "        axs[0].set_yticklabels(ytickslabels)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            fig_avg, axs = plt.subplots(1, len(strain_avg_data), figsize = figsize, sharex = True, sharey = True)\n",
    "            figs2return.append(fig_avg)\n",
    "\n",
    "        for ax, (strain, (df_stat, (df_err_low, df_err_high))) in zip(axs, strain_avg_data.items()):\n",
    "            yy = df_stat[feat].values.copy()\n",
    "            er = df_err[feat].values.copy()\n",
    "\n",
    "            ax.plot(xbins, yy)\n",
    "            ax.fill_between(xbins, yy - er, yy + er, alpha=0.5)\n",
    "            ax.set_title(strain_titles[strain])\n",
    "\n",
    "            #yy = data_avg[feat][stat_type].values.copy()\n",
    "            #invalid = data_avg[feat]['count'].values < min_valid_counts\n",
    "            #yy[invalid] = np.nan\n",
    "            #ax.plot(xbins, yy)\n",
    "            #ax.set_title(strain_titles[strain])\n",
    "\n",
    "        axs[0].set_ylabel(feat_label)\n",
    "\n",
    "        ylims = ax.get_ylim()\n",
    "        for ax in axs:\n",
    "            ax.plot((0,0), ylims, ':k')\n",
    "    \n",
    "    if save_name is not None:\n",
    "        save_name.parent.mkdir(exist_ok = True, parents = True)\n",
    "        with PdfPages(save_name) as pdf:\n",
    "            for fig in figs2return:\n",
    "                pdf.savefig(fig)\n",
    "        \n",
    "    return figs2return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f16f0ea7e94b34beb84c3ad77c228d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c6ab3ca86c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mxbins_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mshort_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIVERGENT_SET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxbins_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mxbins_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ebd34ad9702>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(src_file, events_dir, strains2read, xbins, qbins, fps, abs_ventral_features)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdf_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_by_timebin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mstrain_avg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstrain\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_grouped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6dd2b843d50f>\u001b[0m in \u001b[0;36mgroup_by_timebin\u001b[0;34m(df, xbins, reduce_func, bootstraping_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mbootstraping_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstraping_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0merr_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdf_err_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdf_err_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dd' is not defined"
     ]
    }
   ],
   "source": [
    "#bn = 'worm-eggs-adam-masks+Feggs+roi128+hard-neg-5_clf+unet-simple_maxlikelihood_20190808_151948_adam_lr0.000128_wd0.0_batch64'\n",
    "#events_dir = Path.home() / 'OneDrive - Nexus365/worms/eggs/egg_laying' / bn\n",
    "\n",
    "bn = 'AUG_worm-eggs-adam-masks+Feggs+roi128+hard-neg-5_clf+unet-simple_maxlikelihood_20190808_151948_adam_lr0.000128_wd0.0_batch64'\n",
    "events_dir = Path.home() / 'workspace/WormData/egg_laying/plates/predictions/' / bn\n",
    "\n",
    "src_file = events_dir / 'files_data.pkl'\n",
    "histograms_dir = events_dir / 'histograms'\n",
    "\n",
    "DIVERGENT_SET = ['CB4856', 'N2',  'DL238', 'CX11314', 'MY23', 'JU775', 'JT11398',\n",
    "       'EG4725', 'LKC34', 'ED3017', 'MY16', 'JU258']\n",
    "\n",
    "xbins_short = np.arange(-10, 10.1, 1)\n",
    "short_data = read_data(src_file, events_dir, DIVERGENT_SET, xbins = xbins_short);\n",
    "\n",
    "xbins_long = np.arange(-120, 120.1, 10)\n",
    "long_data = read_data(src_file, events_dir, DIVERGENT_SET, xbins = xbins_long);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feats2plot = ['length', 'd_length']\n",
    "save_name = histograms_dir / 'length-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'length-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);\n",
    "\n",
    "\n",
    "feats2plot = ['speed', 'd_speed']\n",
    "save_name = histograms_dir / 'speed-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'speed-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feats2plot = ['relative_to_head_base_radial_velocity_head_tip', \n",
    "              'relative_to_neck_radial_velocity_head_tip', \n",
    "              'relative_to_body_radial_velocity_head_tip', \n",
    "              'relative_to_body_radial_velocity_neck', \n",
    "              'relative_to_body_radial_velocity_hips', \n",
    "              'relative_to_body_radial_velocity_tail_tip', \n",
    "              'relative_to_hips_radial_velocity_tail_tip', \n",
    "              'relative_to_tail_base_radial_velocity_tail_tip']\n",
    "\n",
    "\n",
    "save_name = histograms_dir / 'radial_velocity-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feats2plot = sorted([x for x in all_feats if 'curvature_std' in x and not x.startswith('d')])\n",
    "# feats2plot = ['curvature_head',\n",
    "#               'curvature_neck',\n",
    "#               'curvature_midbody',\n",
    "#               'curvature_hips', \n",
    "#               'curvature_tail',\n",
    "              \n",
    "#               'curvature_mean_head', \n",
    "#               'curvature_mean_neck', \n",
    "#               'curvature_mean_midbody',\n",
    "#               'curvature_mean_hips',\n",
    "#               'curvature_mean_tail',\n",
    "              \n",
    "#               'curvature_std_head', \n",
    "#               'curvature_std_neck', \n",
    "#               'curvature_std_midbody',\n",
    "#               'curvature_std_hips',\n",
    "#               'curvature_std_tail'\n",
    "#              ]\n",
    "feats2plot = ['curvature_head', 'curvature_std_midbody']\n",
    "save_name = histograms_dir / 'curvature-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'curvature-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feats2plot = ['angular_velocity', \n",
    "              'relative_to_neck_angular_velocity_head_tip', \n",
    "              'relative_to_body_angular_velocity_head_tip', \n",
    "              'relative_to_body_angular_velocity_neck', \n",
    "              'relative_to_body_angular_velocity_hips', \n",
    "              'relative_to_body_angular_velocity_tail_tip', \n",
    "              'relative_to_hips_angular_velocity_tail_tip',\n",
    "              ]\n",
    "\n",
    "save_name = histograms_dir / 'angular_velocity-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'angular_velocity-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2plot = ['dist_from_food_edge']#, 'orientation_food_edge']\n",
    "save_name = histograms_dir / 'food-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'food-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feats2plot = ['width_midbody', \n",
    "              ]\n",
    "\n",
    "save_name = histograms_dir / 'midbody-short.pdf'\n",
    "plot_features(*short_data, feats2plot = feats2plot, xbins = xbins_short, save_name = save_name);\n",
    "\n",
    "save_name = histograms_dir / 'midbody-long.pdf'\n",
    "plot_features(*long_data, feats2plot = feats2plot, xbins = xbins_long, save_name = save_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
